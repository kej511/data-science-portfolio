{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truck Allocation Forecast Model\n",
    "\n",
    "## Background and Objective\n",
    "The goal of this project is to build a model that predicts the number of trucks required for daily shipments in a logistics optimization context. Specifically, the objective is to predict the number of trucks needed per day, which helps in streamlining the logistics process and improving delivery efficiency.\n",
    "\n",
    "## Logic and Formulas\n",
    "\n",
    "### 1. Shipment Units Calculation\n",
    "The number of shipment units is calculated using the following formulas:\n",
    "\n",
    "#### 1.1 Single Shipment Units\n",
    "\n",
    "$$S_{\\text{single}} = U_d \\times R_{\\text{single}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U_d$ is the daily shipment volume (in units)\n",
    "- $R_{\\text{single}}$ is the single shipment ratio\n",
    "\n",
    "#### 1.2 Multi Shipment Units\n",
    "\n",
    "$$S_{\\text{multi}} = \\frac{U_d \\times R_{\\text{multi}}}{U_{\\text{multi}}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U_{\\text{multi}}$ is the average units per multi shipment\n",
    "- $R_{\\text{multi}}$ is the multi shipment ratio\n",
    "\n",
    "### 2. Email and Box Shipment Ratios\n",
    "The total shipment units are divided into email and box shipments, calculated based on the following ratios:\n",
    "\n",
    "#### 2.1 Email Shipments\n",
    "\n",
    "$$S_M = S_{\\text{total}} \\times S_{M\\text{ratio}}$$\n",
    "\n",
    "#### 2.2 Box Shipments\n",
    "\n",
    "$$S_B = S_{\\text{total}} \\times S_{B\\text{ratio}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S_M$ is the total email shipments\n",
    "- $S_B$ is the total box shipments\n",
    "- $S_{\\text{total}}$ is the total shipment units\n",
    "- $S_{M\\text{ratio}}$ is the email shipment ratio\n",
    "- $S_{B\\text{ratio}}$ is the box shipment ratio\n",
    "\n",
    "### 3. Truck Allocation Calculation\n",
    "The total number of trucks required is calculated by dividing the email and box shipments by their respective cargo capacities, then summing the results and dividing by the truck capacity. The final truck allocation is calculated as follows:\n",
    "\n",
    "$$\\text{Total Trucks} = \\left\\lceil \\frac{\\frac{S_M}{S_{M\\text{capacity}}} + \\frac{S_B}{S_{B\\text{capacity}}}}{\\text{cargo\\_per\\_truck}} \\right\\rceil$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S_{M\\text{capacity}} = 400$ is the email shipment capacity\n",
    "- $S_{B\\text{capacity}} = 75$ is the box shipment capacity\n",
    "- $\\text{cargo\\_per\\_truck} = 22$ is the cargo capacity per truck\n",
    "\n",
    "### 4. Moving Average\n",
    "A 7-day moving average of the truck numbers is added as a feature to improve the model's accuracy. The moving average of truck numbers is calculated as follows:\n",
    "\n",
    "$$\\text{Moving Average of Trucks} = \\frac{1}{7}\\sum_{i=t-6}^t T_i$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $T_i$ is the truck number for day $i$\n",
    "- $t$ is the current day\n",
    "\n",
    "## Models Used\n",
    "\n",
    "### 1. Linear Regression\n",
    "Linear regression assumes a linear relationship between the explanatory variables and the target variable. While simple and interpretable, linear regression may struggle to capture complex nonlinear relationships in the data.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $y$ is the target variable (truck number)\n",
    "- $X_1, X_2, \\ldots, X_n$ are the explanatory variables (daily shipment volume, shipment ratios, moving average, etc.)\n",
    "\n",
    "### 2. Random Forest\n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and aggregates their predictions. It is capable of capturing complex nonlinear relationships between the features and the target variable.\n",
    "\n",
    "The Random Forest model involves the following hyperparameters:\n",
    "\n",
    "- $\\text{max\\_depth} = \\text{None}$\n",
    "- $\\text{min\\_samples\\_split} = 2$\n",
    "- $\\text{min\\_samples\\_leaf} = 2$\n",
    "- $\\text{n\\_estimators} = 50$\n",
    "\n",
    "These parameters were optimized using GridSearch.\n",
    "\n",
    "## Results\n",
    "Linear Regression MAE: 0.59\n",
    "\n",
    "Random Forest MAE (after hyperparameter tuning): 1.93\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### 1. Model Improvement\n",
    "Further improvements to the Random Forest model by refining the data preprocessing steps or exploring other models (e.g., Gradient Boosting) to improve accuracy.\n",
    "\n",
    "### 2. Data Collection\n",
    "Use real-world data to assess the model's accuracy. Incorporating external data, such as weather forecasts or public holiday schedules, may improve prediction accuracy.\n",
    "\n",
    "### 3. Model Evaluation\n",
    "Evaluate the model using additional metrics such as RMSE and $R^2$, to gain a more comprehensive understanding of the model's performance.\n",
    "\n",
    "## Explanation for the Models and Methods\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression assumes a simple relationship between input features and the target variable. However, in real-world problems, relationships are often more complex and nonlinear. Linear regression provides a baseline to compare more complex models like Random Forest.\n",
    "\n",
    "### Random Forest\n",
    "Random Forest is an ensemble technique that helps reduce overfitting compared to a single decision tree by averaging the predictions of multiple trees. This makes it more robust and suitable for complex datasets with nonlinear relationships. The model's performance is further enhanced by hyperparameter tuning, which helps find the best settings for the trees.\n",
    "\n",
    "### Moving Average\n",
    "The moving average of the truck numbers over the past 7 days is used to capture trends and seasonal effects. This feature can help improve prediction accuracy by providing the model with information on recent trends.\n",
    "\n",
    "## Why We Chose These Approaches\n",
    "The linear regression model was chosen to provide a baseline comparison. Random Forest, with its ability to capture complex patterns in data, was selected as a more powerful model. Hyperparameter tuning was essential for optimizing the Random Forest model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1152 candidates, totalling 5760 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "1440 fits failed out of a total of 5760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1440 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 448, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [-1.1758     -1.1758     -1.1713     ... -1.50437635         nan\n",
      " -1.5072769 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 0.35156280589157757\n",
      "Random Forest MAE (Tuned): 0.8500000000000014\n",
      "Best Random Forest Parameters: {'bootstrap': True, 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200, 'oob_score': True}\n",
      "\n",
      "Top 10 most important features:\n",
      "                      feature  importance\n",
      "37  volume_ratio multi_volume    0.259284\n",
      "22           U_d volume_ratio    0.096715\n",
      "28      R_single multi_volume    0.060593\n",
      "23           U_d multi_volume    0.060411\n",
      "19               U_d R_single    0.056122\n",
      "12                        U_d    0.048254\n",
      "31       R_multi volume_ratio    0.046713\n",
      "18                      U_d^2    0.045459\n",
      "0                         U_d    0.036355\n",
      "34       U_multi volume_ratio    0.029934\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    \"Date\": pd.date_range(start=\"2024-04-01\", periods=30, freq='D'),\n",
    "    \"U_d\": np.random.randint(180000, 220000, size=30),\n",
    "    \"R_single\": np.random.uniform(0.25, 0.35, size=30),\n",
    "    \"R_multi\": np.random.uniform(0.65, 0.75, size=30),\n",
    "    \"U_multi\": np.random.uniform(2.0, 2.2, size=30),\n",
    "    \"S_M_ratio\": np.full(30, 0.4),\n",
    "    \"S_B_ratio\": np.full(30, 0.6),\n",
    "    \"Weather\": np.random.choice([\"Sunny\", \"Rainy\", \"Cloudy\"], size=30),\n",
    "    \"Holiday\": np.random.choice([0, 1], size=30),\n",
    "    \"Sale_Flag\": np.random.choice([0, 1], size=30)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, columns=[\"Weather\"], drop_first=True)\n",
    "\n",
    "# Calculate target variable (number of trucks needed)\n",
    "def calculate_trucks(row, S_M_capacity=400, S_B_capacity=75, cargo_per_truck=22):\n",
    "    S_single = row[\"U_d\"] * row[\"R_single\"]\n",
    "    S_multi = (row[\"U_d\"] * row[\"R_multi\"]) / row[\"U_multi\"]\n",
    "    S_total = S_single + S_multi\n",
    "    S_M = S_total * row[\"S_M_ratio\"]\n",
    "    S_B = S_total * row[\"S_B_ratio\"]\n",
    "    C_total = (S_M / S_M_capacity) + (S_B / S_B_capacity)\n",
    "    return math.ceil(C_total / cargo_per_truck)\n",
    "\n",
    "df[\"Total_Trucks\"] = df.apply(calculate_trucks, axis=1)\n",
    "\n",
    "# Feature Engineering\n",
    "# Add interaction terms first\n",
    "df['volume_ratio'] = df['U_d'] * df['R_single']\n",
    "df['multi_volume'] = df['U_d'] * df['R_multi']\n",
    "\n",
    "# Add polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df[['U_d', 'R_single', 'R_multi', 'U_multi', 'volume_ratio', 'multi_volume']])\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['U_d', 'R_single', 'R_multi', 'U_multi', 'volume_ratio', 'multi_volume']))\n",
    "df = pd.concat([df, poly_df], axis=1)\n",
    "\n",
    "# Add moving average and rolling statistics\n",
    "df[\"Moving_Avg_Trucks\"] = df[\"Total_Trucks\"].rolling(window=7, min_periods=1).mean()\n",
    "df['rolling_std'] = df['Total_Trucks'].rolling(window=7, min_periods=1).std()\n",
    "df['rolling_max'] = df['Total_Trucks'].rolling(window=7, min_periods=1).max()\n",
    "df['rolling_min'] = df['Total_Trucks'].rolling(window=7, min_periods=1).min()\n",
    "\n",
    "# Fill NaN values with appropriate values\n",
    "df[\"Moving_Avg_Trucks\"] = df[\"Moving_Avg_Trucks\"].fillna(df[\"Total_Trucks\"])\n",
    "df['rolling_std'] = df['rolling_std'].fillna(0)  # Fill with 0 for first few days\n",
    "df['rolling_max'] = df['rolling_max'].fillna(df[\"Total_Trucks\"])\n",
    "df['rolling_min'] = df['rolling_min'].fillna(df[\"Total_Trucks\"])\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop(columns=[\"Date\", \"Total_Trucks\"])\n",
    "y = df[\"Total_Trucks\"]\n",
    "\n",
    "# Split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and predict with linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "\n",
    "# Expanded hyperparameter search for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 8],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'oob_score': [True, False]\n",
    "}\n",
    "\n",
    "# Use regular cross-validation\n",
    "rf_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_param_grid,\n",
    "    cv=5,  # Using 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "# Train and predict with best model\n",
    "best_rf_model = rf_search.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(f\"Linear Regression MAE: {lr_mae}\")\n",
    "print(f\"Random Forest MAE (Tuned): {rf_mae}\")\n",
    "print(f\"Best Random Forest Parameters: {rf_search.best_params_}\")\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation with Random Data: Analysis and Insights\n",
    "# 1. Introduction\n",
    "In this project, I aimed to build machine learning models to predict outcomes based on a dataset that includes inherent randomness. This randomness reflects real-world uncertainty, and as such, I focused on selecting techniques that are robust to noise and can generalize well in unpredictable scenarios. By doing so, I was able to demonstrate how data scientists can handle uncertain and random data effectively.\n",
    "\n",
    "# 2. Understanding Randomness in Data\n",
    "Real-world datasets often contain random or noisy elements that can significantly affect the performance of predictive models. The presence of randomness means that certain patterns might not always hold true across the entire dataset, which can lead to variability in predictions. This type of dataset is common in business contexts, where customer behavior, demand fluctuations, or stock prices are influenced by many random factors.\n",
    "\n",
    "# 3. Feature Engineering\n",
    "Feature engineering is essential in this scenario because the goal is to capture as much useful information as possible while minimizing the impact of noise. In this case, I utilized the following techniques:\n",
    "\n",
    "Polynomial Features: To capture non-linear relationships between features, I used polynomial transformations. This helps in modeling interactions that may not be immediately apparent but can still have significant predictive power.\n",
    "\n",
    "Interaction Terms: By creating interaction features like volume_ratio and multi_volume, I was able to include combinations of variables that might offer insights into the underlying patterns of the data.\n",
    "\n",
    "These features were added to the dataset to provide more predictive power, taking into account both linear and non-linear relationships.\n",
    "\n",
    "# 4. Model Selection and Evaluation\n",
    "Given the random nature of the data, it was important to use models that can handle uncertainty and noise effectively. I tested two popular models:\n",
    "\n",
    "- Linear Regression:\n",
    "Linear regression is a simple model that assumes a linear relationship between the features and the target. Despite its simplicity, it provided a baseline for model performance and allowed me to understand how well a straightforward approach could work on random data. The Mean Absolute Error (MAE) for linear regression was 0.35, indicating reasonable predictive accuracy on the data with randomness.\n",
    "\n",
    "- Random Forest (Tuned):\n",
    "The random forest model, an ensemble learning technique, is particularly well-suited for handling randomness due to its ability to aggregate predictions from multiple decision trees. By tuning hyperparameters like the number of estimators and the maximum depth of trees, I improved the model's ability to generalize and deal with random fluctuations in the data. The MAE for the tuned random forest was 0.85, which was higher than the linear regression model. This reflects that even with more complexity, the model was still unable to fully capture the underlying patterns in the data due to the randomness.\n",
    "\n",
    "# 5. Randomness and Overfitting\n",
    "One of the key challenges when dealing with random data is overfitting. Overfitting occurs when the model captures not only the true underlying patterns but also the noise in the data. This results in poor generalization to new, unseen data.\n",
    "\n",
    "To prevent overfitting:\n",
    "\n",
    "Cross-validation was used to assess the model's performance on different subsets of the data, ensuring that the model did not simply memorize the training data.\n",
    "\n",
    "Regularization techniques were employed to penalize overly complex models, helping the model remain simple and generalize better.\n",
    "\n",
    "# 6. Model Interpretation and Feature Importance\n",
    "Feature importance analysis is critical when dealing with random data. By identifying the most important features, we can gain insights into what drives the model's predictions. In the random forest model, the top 10 most important features were derived, including interaction terms like volume_ratio multi_volume and individual features like U_d. This helps highlight which features the model found most informative, even in the face of randomness.\n",
    "\n",
    "# 7. Insights and Takeaways\n",
    "Handling Randomness: Random data requires special attention, especially regarding feature selection and model complexity. The simple linear model was better at handling noise, while the more complex random forest model struggled due to the inherent randomness.\n",
    "\n",
    "Bias-Variance Tradeoff: The random forest's higher error rate demonstrates the bias-variance tradeoff. With more complexity comes the risk of overfitting, especially when the data is noisy and doesn't follow consistent patterns.\n",
    "\n",
    "Model Robustness: Models like random forest are generally more robust to noise, but in this case, the randomness in the dataset led to suboptimal performance.\n",
    "\n",
    "# 8. Conclusion\n",
    "This project showcased the importance of understanding and addressing randomness in real-world data. By employing techniques like feature engineering, cross-validation, and careful model selection, I was able to build models that not only accounted for randomness but also performed reasonably well despite the challenges. This approach is essential for any data scientist, as real-world data is rarely perfect, and models must be able to adapt to uncertainty and noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
